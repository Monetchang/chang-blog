---
title: "ã€è§£å¯†æºç ã€‘ RAGFlow åˆ‡åˆ†æœ€ä½³å®è·µ-ä¸Šä¼ ä¸è§£æå…¨æµç¨‹"
date: 2025-10-16T20:39:10+08:00
draft: true
tags: ["æºç ","æŠ€æœ¯",RAG]
categories: ["RAGFlow"]
---

*æœ¬ç³»åˆ—æ–‡ç« å¸¦ä½ ä»æºç è§’åº¦æ·±åº¦å‰–æ RAGFlowï¼Œä»æ–‡ä»¶ä¸Šä¼ ã€è§£æã€åˆ‡åˆ†ã€å‘é‡åŒ–åˆ°æœ€ç»ˆå…¥åº“ã€‚æœ¬æ–‡èšç„¦äºæ–‡æ¡£è§£æä¸åˆ‡åˆ†çš„å…¨æµç¨‹æ¦‚è¿°ï¼Œä¸ºç†è§£æ•´ä¸ª RAGFlow æµç¨‹æ‰“ä¸‹åŸºç¡€ã€‚*

# å¼•è¨€

éšç€å¤§æ¨¡å‹åœ¨ä¼ä¸šåº”ç”¨ä¸­çš„è½åœ°åŠ é€Ÿï¼ŒRAGï¼ˆRetrieval-Augmented Generationï¼‰æŠ€æœ¯é€æ¸æˆä¸ºçŸ¥è¯†é—®ç­”ç³»ç»Ÿçš„æ ¸å¿ƒã€‚
RAGFlow æ˜¯ä¸€ä¸ªé¢å‘å·¥ç¨‹åŒ–çš„ RAG å·¥ä½œæµæ¡†æ¶ï¼Œå®ƒæä¾›äº†ä»æ–‡æ¡£è§£æã€å‘é‡åŒ–åˆ°æ£€ç´¢é—®ç­”çš„ä¸€æ•´å¥—æµç¨‹ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€Ÿæ„å»ºå¯æ‰©å±•çš„çŸ¥è¯†å¢å¼ºç³»ç»Ÿã€‚

æœ¬ç¯‡æ–‡ç« å±äºã€ŠRAGFlow æºç è§£å¯†ã€‹ç³»åˆ—ç¬¬ä¸€æœŸï¼Œç›®æ ‡æ˜¯ï¼šç†è§£æ–‡æ¡£ä¸Šä¼ ä¸è§£æçš„æ•´ä½“æµç¨‹ã€‚

# çœæµç‰ˆï¼ˆå¿«é€Ÿç†è§£æ ¸å¿ƒé€»è¾‘ï¼‰

**å¦‚æœä½ åªæƒ³å¿«é€Ÿç†è§£æµç¨‹ï¼Œè¿™é‡Œæ˜¯æœ€æ ¸å¿ƒçš„å†…å®¹ğŸ‘‡**

**æ ¸å¿ƒç›®æ ‡**ï¼šææ‡‚ RAGFlow å¦‚ä½•å°†ä½ ä¸Šä¼ çš„ä¸€ä¸ªåŸå§‹æ–‡æ¡£ï¼ˆå¦‚PDFã€PPTï¼‰ï¼Œå˜æˆå¯ä»¥è¢«æ£€ç´¢å’Œé—®ç­”çš„çŸ¥è¯†ç‰‡æ®µã€‚

**å…¨è¿‡ç¨‹å…­æ­¥æµç¨‹å›¾**ï¼š
`0. ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£` â†’ `1. å®šä½çŸ¥è¯†åº“` â†’ `2. å­˜å‚¨å¹¶åˆ†ææ–‡æ¡£` â†’ `3. è°ƒç”¨è§£æå™¨è¿›è¡Œæ™ºèƒ½åˆ‡å—` â†’ `4. ä¸ºæ–‡æœ¬/å›¾ç‰‡/æ€ç»´å¯¼å›¾ç”Ÿæˆç´¢å¼•` â†’ `5. å‘é‡åŒ–å¹¶å­˜å…¥å‘é‡æ•°æ®åº“` â†’ `6. è¿”å›æ–‡æ¡£ID`

**ä¸Šä¼ æ–‡æ¡£**
- **åŠŸèƒ½**ï¼šæ¥æ”¶ç”¨æˆ·ä¸Šä¼ çš„ PDF / DOCX / Markdown / å›¾ç‰‡æ–‡ä»¶ï¼Œè‡ªåŠ¨è¯†åˆ«æ–‡ä»¶ç±»å‹ä¸ç¼–ç æ ¼å¼ã€‚  
- **æ ¸å¿ƒå‡½æ•° / è°ƒç”¨ç‚¹**ï¼š`/upload_and_parse`  

**ç»‘å®šçŸ¥è¯†åº“**
- **åŠŸèƒ½**ï¼šæ ¹æ®ç”¨æˆ·ä¸Šä¸‹æ–‡æˆ–é€‰æ‹©ç¡®å®šç›®æ ‡çŸ¥è¯†åº“ï¼Œå®ç°æ–‡æ¡£ä¸çŸ¥è¯†åº“çš„ç»‘å®šã€‚  
- **æ ¸å¿ƒå‡½æ•° / è°ƒç”¨ç‚¹**ï¼š  
  - `ConversationService.get_by_id()`  
  - `KnowledgebaseService.get_by_id()`  

**æ–‡ä»¶å­˜å‚¨**
- **åŠŸèƒ½**ï¼šå°†æ–‡æ¡£ä¿å­˜è‡³å¯¹è±¡å­˜å‚¨ï¼ˆMinIO / S3 ç­‰ï¼‰ï¼Œæå–åŸºç¡€å…ƒæ•°æ®ï¼ˆç±»å‹ã€å¤§å°ã€é¡µæ•°ç­‰ï¼‰ã€‚  
- **æ ¸å¿ƒå‡½æ•° / è°ƒç”¨ç‚¹**ï¼š`FileService.upload_document()`  

**å†…å®¹è§£æ**
- **åŠŸèƒ½**ï¼šæ ¹æ®æ–‡ä»¶ç±»å‹è°ƒç”¨å¯¹åº”è§£æå™¨ï¼Œå°†å†…å®¹åˆ‡åˆ†ä¸ºè¯­ä¹‰å—ï¼ˆchunksï¼‰ï¼Œç”Ÿæˆç»“æ„åŒ–å†…å®¹ã€‚  
- **æ ¸å¿ƒå‡½æ•° / è°ƒç”¨ç‚¹**ï¼š`FACTORY.get(...).chunk()`  
- **è®¾è®¡äº®ç‚¹**ï¼š  
  - å¯¹éå›¾ç‰‡ç±»æ–‡æ¡£ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ç”Ÿæˆ **æ€ç»´å¯¼å›¾ï¼ˆMindMapï¼‰** ä½œä¸ºå…³è”å†…å®¹è¿›è¡Œå­˜å‚¨ã€‚  
  - å¯æ‰©å±•å¤šç§è§£æå™¨ï¼ˆPDFParserã€MarkdownParserã€DocxParserã€ImageParserï¼‰ã€‚

**ç´¢å¼•ç”Ÿæˆ**
- **åŠŸèƒ½**ï¼šä¸ºæ¯ä¸ªå†…å®¹å—åˆ›å»ºå”¯ä¸€ç´¢å¼•ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡ã€æ€ç»´å¯¼å›¾ç­‰å¤šæ¨¡æ€å†…å®¹ç»Ÿä¸€æ£€ç´¢ã€‚  
- **æ ¸å¿ƒå‡½æ•° / è°ƒç”¨ç‚¹**ï¼šå†…éƒ¨ç´¢å¼•æ„å»ºé€»è¾‘ï¼ˆ`IndexService.build_index()`ï¼‰  

**å‘é‡åŒ–å…¥åº“**
- **åŠŸèƒ½**ï¼šè°ƒç”¨ Embedding æ¨¡å‹å°†æ–‡æœ¬å—è½¬æ¢ä¸ºå‘é‡ï¼Œå¹¶å­˜å…¥å‘é‡æ•°æ®åº“ï¼ˆES / infinity / opensearchï¼‰ã€‚  
- **æ ¸å¿ƒå‡½æ•° / è°ƒç”¨ç‚¹**ï¼š  
  - `LLMBundle.encode()`  
  - `docStoreConn.insert()`  

**è¿”å›ç»“æœ**
- **åŠŸèƒ½**ï¼šè¿”å›æ–‡æ¡£å”¯ä¸€ IDã€å‘é‡æ•°é‡åŠçŠ¶æ€ä¿¡æ¯ï¼Œè¡¨ç¤ºå…¥åº“å®Œæˆã€‚  
- **æ ¸å¿ƒå‡½æ•° / è°ƒç”¨ç‚¹**ï¼š`DocumentService.increment_chunk_num()`  

# æ‰‹æ’•ç‰ˆï¼ˆæºç æ·±è§£ï¼‰

**æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ·±å…¥ä»£ç ç»†èŠ‚ï¼Œé€ä¸€æ‹†è§£"çœæµç‰ˆ"ä¸­çš„æ¯ä¸€æ­¥**

## å…¥å£ä¸è·¯ç”±

åœ¨æä¾›ç»™å‰ç«¯çš„æ¥å£ä¸­ï¼Œæœ‰ä¸ª `/upload_and_parse` æ¥å£ï¼Œé€šè¿‡æ¥å£è¯­ä¹‰å¯ä»¥çŸ¥é“è¿™ä¸ªæ¥å£çš„åŠŸèƒ½æ˜¯ç”¨æ¥æ¥æ”¶ä¸Šä¼ æ–‡æ¡£å¹¶è¿›è¡Œè§£æçš„

```python
@manager.route("/upload_and_parse", methods=["POST"])  # noqa: F821
@login_required
@validate_request("conversation_id")
def upload_and_parse():
  if "file" not in request.files:
      return get_json_result(data=False, message="No file part!", code=settings.RetCode.ARGUMENT_ERROR)

  file_objs = request.files.getlist("file")
  for file_obj in file_objs:
      if file_obj.filename == "":
          return get_json_result(data=False, message="No file selected!", code=settings.RetCode.ARGUMENT_ERROR)

  doc_ids = doc_upload_and_parse(request.form.get("conversation_id"), file_objs, current_user.id)

  return get_json_result(data=doc_ids)
```

æ•´ä¸ªå‡½æ•°ä¸­æœ€ä¸»è¦çš„å‡½æ•°æ˜¯ `doc_upload_and_parse`

è·³è½¬åˆ° `doc_upload_and_parse` å‡½æ•°åï¼Œå¯ä»¥çœ‹åˆ°ç¬¬ä¸€éƒ¨åˆ†ä»£ç 

## çŸ¥è¯†åº“å…³è”
é€šè¿‡ conversation_id è·å–å…³è”çš„ Knowledgebaseï¼ˆçŸ¥è¯†åº“ï¼‰

```python
e, conv = ConversationService.get_by_id(conversation_id)
if not e:
    e, conv = API4ConversationService.get_by_id(conversation_id)
assert e, "Conversation not found!"

e, dia = DialogService.get_by_id(conv.dialog_id)
if not dia.kb_ids:
    raise LookupError("No knowledge base associated with this conversation. "
                      "Please add a knowledge base before uploading documents")
kb_id = dia.kb_ids[0]
e, kb = KnowledgebaseService.get_by_id(kb_id)
if not e:
    raise LookupError("Can't find this knowledgebase!")

```

ğŸ’¡ è¯´æ˜ï¼šçŸ¥è¯†åº“åœ¨ RAGFlow ä¸­çš„å«ä¹‰ï¼Œè¿™é‡Œä¸åšé‡ç‚¹ä»‹ç»ï¼Œå¯ä»¥ç†è§£ä¸€ä¸ªç‹¬ç«‹çš„çŸ¥è¯†é›†åˆï¼ŒåŒ…æ‹¬å¤šä¸ªæ–‡æ¡£ï¼Œæ¯ä¸ªå¯¹è¯ï¼ˆConversationï¼‰éƒ½ç»‘å®šä¸€ä¸ªæˆ–å¤šä¸ªçŸ¥è¯†åº“ï¼Œä»¥é™å®šæ£€ç´¢èŒƒå›´ã€‚ã€‚

## æ–‡ä»¶å­˜å‚¨ä¸ç™»è®°
æ–‡ä»¶ä¸Šä¼ é€»è¾‘ç”±`FileService.upload_document()`å®ç°ã€‚å°†æ–‡ä»¶å­˜å‚¨åˆ°å¯¹åº”çš„çŸ¥è¯†åº“ä¸­ï¼Œå¹¶è¿”å›ç›¸åº”æ–‡ä»¶ä¿¡æ¯ã€‚

```python
err, files = FileService.upload_document(kb, file_objs, user_id)
```

é‡ç‚¹å…³æ³¨ä¸Šä¼ åè¿”å›çš„ files ç»“æ„ä½“

```python
doc = {
    "id": doc_id,
    "kb_id": kb.id,
    "parser_id": self.get_parser(filetype, filename, kb.parser_id),
    "parser_config": kb.parser_config,
    "created_by": user_id,
    "type": filetype,
    "name": filename,
    "suffix": Path(filename).suffix.lstrip("."),
    "location": location,
    "size": len(blob),
    "thumbnail": thumbnail_location,
}
DocumentService.insert(doc)
```

ğŸ” å…³é”®ç‚¹ï¼š
- parser_id å†³å®šæ–‡ä»¶ä½¿ç”¨å“ªç§è§£æå™¨ã€‚
- æ¯ä¸ªçŸ¥è¯†åº“å¯é…ç½®é»˜è®¤è§£æå™¨ï¼ˆPDFã€å›¾ç‰‡ã€éŸ³é¢‘ç­‰ç±»å‹å„ä¸åŒï¼‰ã€‚

## æ ¸å¿ƒè§£æä¸åˆ†å—
é€šè¿‡è§£æå™¨å·¥å‚ï¼ˆFACTORYï¼‰åŠ¨æ€é€‰æ‹©ä¸åŒçš„è§£æå™¨ï¼Œå¯¹ä¸åŒæ ¼å¼æ–‡ä»¶è¿›è¡Œè§£æã€‚

```python
FACTORY = {
    ParserType.PRESENTATION.value: presentation,
    ParserType.PICTURE.value: picture,
    ParserType.AUDIO.value: audio,
    ParserType.EMAIL.value: email
}
```

ä¸»å¾ªç¯é€»è¾‘å¦‚ä¸‹ï¼š
```python
parser_config = {"chunk_token_num": 4096, "delimiter": "\n!?;ã€‚ï¼›ï¼ï¼Ÿ", "layout_recognize": "Plain Text"}
...
for d, blob in files:
    kwargs = {
        "callback": dummy,
        "parser_config": parser_config,
        "from_page": 0,
        "to_page": 100000,
        "tenant_id": kb.tenant_id,
        "lang": kb.language
    }
    threads.append(exe.submit(
        FACTORY.get(d["parser_id"], naive).chunk,
        d["name"], blob, **kwargs
    ))

```

é€šè¿‡ FACTORY å­—å…¸å¯ä»¥çœ‹åˆ°ä¸åŒçš„ ParserType å€¼å¯¹åº”ä¸åŒçš„è§£ææ–¹å¼ Presentationï¼ˆPPTï¼‰ï¼Œ Pictureï¼ˆå›¾ç‰‡ï¼‰ï¼ŒAudioï¼ˆéŸ³é¢‘ï¼‰ï¼ŒEmailï¼ˆé‚®ä»¶ï¼‰ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œå°±ç”¨é»˜è®¤çš„ Naive è§£æã€‚

## å†…å®¹å¢å¼ºä¸ç´¢å¼•
æ¯ä¸ª chunk ä¼šç”Ÿæˆä¸€ä¸ªæ–‡æ¡£ç‰‡æ®µï¼Œèµ‹äºˆå”¯ä¸€ idï¼Œä»¥åŠå…¶ä»– metadataã€‚

```python
for (docinfo, _), th in zip(files, threads):
    docs = []
    for ck in th.result():
        d = deepcopy(doc)
        d.update(ck)
        d["id"] = xxhash.xxh64((ck["content_with_weight"] + str(d["doc_id"])).encode("utf-8")).hexdigest()
```

å¦‚æœ chunk å­˜åœ¨å›¾ç‰‡ï¼Œåˆ™ä¼šå°†å›¾ç‰‡è½¬æ¢æˆå­—èŠ‚æµçš„å½¢å¼å•ç‹¬å­˜å‚¨ï¼Œå¹¶å»ºç«‹ç´¢å¼•ã€‚

```python
output_buffer = BytesIO()
if isinstance(d["image"], bytes):
    output_buffer = BytesIO(d["image"])
else:
    d["image"].save(output_buffer, format='JPEG')

STORAGE_IMPL.put(kb.id, d["id"], output_buffer.getvalue())
d["img_id"] = "{}-{}".format(kb.id, d["id"])
```

å¦‚æœä¸æ˜¯å›¾ç‰‡ç±»å‹æ–‡æ¡£ï¼Œåˆ™ä¼šè°ƒç”¨å·¥å…·ç”Ÿæˆæ€ç»´å¯¼å›¾çš„ä½œä¸ºç›¸å…³å†…å®¹è¿›è¡Œå­˜å‚¨ã€‚

```python
if parser_ids[doc_id] != ParserType.PICTURE.value:
from graphrag.general.mind_map_extractor import MindMapExtractor
mindmap = MindMapExtractor(llm_bdl)
try:
    mind_map = trio.run(mindmap, [c["content_with_weight"] for c in docs if c["doc_id"] == doc_id])
    mind_map = json.dumps(mind_map.output, ensure_ascii=False, indent=2)
    if len(mind_map) < 32:
        raise Exception("Few content: " + mind_map)
    cks.append({
        "id": get_uuid(),
        "doc_id": doc_id,
        "kb_id": [kb.id],
        "docnm_kwd": doc_nm[doc_id],
        "title_tks": rag_tokenizer.tokenize(re.sub(r"\.[a-zA-Z]+$", "", doc_nm[doc_id])),
        "content_ltks": rag_tokenizer.tokenize("summary summarize æ€»ç»“ æ¦‚å†µ file æ–‡ä»¶ æ¦‚æ‹¬"),
        "content_with_weight": mind_map,
        "knowledge_graph_kwd": "mind_map"
    })
```

## å‘é‡åŒ–ä¸æŒä¹…åŒ–

å°† chunk é€šè¿‡ embedding æ¨¡å‹è¿›è¡Œå‘é‡åŒ–å¹¶å­˜å‚¨åœ¨ d ç»“æ„ä½“ä¸­ã€‚
```python
embd_mdl = LLMBundle(kb.tenant_id, LLMType.EMBEDDING, llm_name=kb.embd_id, lang=kb.language)
def embedding(doc_id, cnts, batch_size=16):
	  nonlocal embd_mdl, chunk_counts, token_counts
	  vects = []
	  for i in range(0, len(cnts), batch_size):
	      vts, c = embd_mdl.encode(cnts[i: i + batch_size])
	      vects.extend(vts.tolist())
	      chunk_counts[doc_id] += len(cnts[i:i + batch_size])
	      token_counts[doc_id] += c
	  return vects
vects = embedding(doc_id, [c["content_with_weight"] for c in cks])
```
éšåå†™å…¥å‘é‡æ•°æ®åº“ï¼ˆå¦‚ Elasticsearchï¼‰ï¼š

```python
if not settings.docStoreConn.indexExist(idxnm, kb_id):
    settings.docStoreConn.createIdx(idxnm, kb_id, len(vects[0]))
	      
settings.docStoreConn.insert(cks[b:b + es_bulk_size], idxnm, kb_id)
```

åœ¨å‘é‡æ•°æ®åº“ä¸­åˆ›å»ºç´¢å¼•è¿›è¡Œå­˜å‚¨ï¼Œä»¥ä¸‹æ˜¯ç³»ç»Ÿå†…ç½®æ”¯æŒçš„å‘é‡æ•°æ®åº“ï¼Œåœ¨ç³»ç»Ÿåˆå§‹åŒ–æ—¶é»˜è®¤ä½¿ç”¨çš„æ˜¯ Elasticsearchã€‚

```python
DOC_ENGINE = os.environ.get("DOC_ENGINE", "elasticsearch")
# DOC_ENGINE = os.environ.get('DOC_ENGINE', "opensearch")
lower_case_doc_engine = DOC_ENGINE.lower()
if lower_case_doc_engine == "elasticsearch":
    docStoreConn = rag.utils.es_conn.ESConnection()
elif lower_case_doc_engine == "infinity":
    docStoreConn = rag.utils.infinity_conn.InfinityConnection()
elif lower_case_doc_engine == "opensearch":
    docStoreConn = rag.utils.opensearch_conn.OSConnection()
else:
    raise Exception(f"Not supported doc engine: {DOC_ENGINE}")
```

## è¿”å›ç»“æœ
æ›´æ–° chunk ä¿¡æ¯ï¼Œå¹¶è¿”å›å¯¹åº”ä¸Šä¼ æ–‡æ¡£çš„ id åˆ—è¡¨ã€‚

```python
DocumentService.increment_chunk_num(
    doc_id, kb.id, token_counts[doc_id], chunk_counts[doc_id], 0)
return [d["id"] for d, _ in files]
```
# ä¸‹æœŸé¢„å‘Š
ä¸‹æœŸæˆ‘ä»¬å°†æ­£å¼èµ°è¿› RAGFlow çš„æ ¸å¿ƒè§£æå™¨ä½“ç³»ï¼Œèšç„¦é»˜è®¤çš„ Naive Parserã€‚