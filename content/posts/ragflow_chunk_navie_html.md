---
title: "ã€è§£å¯†æºç ã€‘ RAGFlow åˆ‡åˆ†æœ€ä½³å®è·µ- naive parser è¯­ä¹‰åˆ‡å—ï¼ˆhtml & json & doc ç¯‡ï¼‰"
date: 2025-10-27T10:15:00+08:00
draft: false
tags: ["æºç ","æŠ€æœ¯","RAG"]
categories: ["RAGFlow"]
---

# å¼•è¨€
åœ¨ RAGFlow çš„å¤šæ–‡æ¡£è§£æä½“ç³»ä¸­ï¼ŒHTMLã€JSON ä¸ DOC ä¸‰ç±»æ–‡æ¡£å…·æœ‰å¤©ç„¶çš„ç»“æ„åŒ–ç‰¹æ€§ã€‚  
ç›¸è¾ƒäº PDFã€Markdown ç­‰å¤æ‚è¾“å…¥ï¼Œå®ƒä»¬çš„è¯­ä¹‰è¾¹ç•Œæ›´æ¸…æ™°ã€å™ªå£°æ›´å°‘ã€è§£æè·¯å¾„æ›´çŸ­ã€‚  

- **HTML** æ–‡ä»¶å…·å¤‡å¼ºæ ‡è®°æ€§ï¼Œå¯é€šè¿‡ DOM é€’å½’è§£æå±‚æ¬¡ç»“æ„ï¼›  
- **JSON / JSONL** æ–‡ä»¶æœ¬èº«å°±æ˜¯ç»“æ„åŒ–æ•°æ®ï¼Œé€‚åˆç›´æ¥åˆ‡åˆ†ä¸ºå±‚çº§åŒ–çš„ chunksï¼›  
- **DOC** åˆ™å±äºé—ç•™æ ¼å¼ï¼Œé€šè¿‡ Tika æå–æ–‡æœ¬ä»ç„¶å…·æœ‰è¾ƒé«˜å…¼å®¹æ€§ã€‚  

RAGFlow åœ¨è®¾è®¡ naive parser çš„è¿‡ç¨‹ä¸­ï¼Œä¸ºè¿™ä¸‰ç±»ç»“æ„åŒ–æ–‡æ¡£å®šåˆ¶äº†ä¸åŒçš„è§£æç­–ç•¥ï¼š  
HTML ä»¥æ ‡ç­¾ä¸ºæ ¸å¿ƒè¿›è¡Œè¯­ä¹‰åˆ†å±‚ï¼›JSON åˆ™ä»¥è·¯å¾„éå†å®ç°â€œè¯­ä¹‰å—åˆ‡åˆ†â€ï¼›DOC ä½¿ç”¨é€šç”¨è§£æå™¨å¿«é€Ÿæå–æ–‡æœ¬å†…å®¹ã€‚  
è¿™äº›ç­–ç•¥å…±åŒç»„æˆäº† RAGFlow çš„â€œè½»ç»“æ„åŒ–è§£æå¼•æ“â€ï¼Œè®©æ¨¡å‹èƒ½é«˜æ•ˆå¸æ”¶äººç±»çŸ¥è¯†çš„ä¸åŒè¡¨ç°å½¢å¼ã€‚
# çœæµç‰ˆ
RAGFlow å¯¹ HTMLã€JSONã€DOC ä¸‰ç±»æ–‡æ¡£é‡‡ç”¨äº†â€œè½»ç»“æ„åŒ–è§£æ + åŠ¨æ€åˆ‡å—â€ç­–ç•¥ï¼Œå®ç°é«˜æ•ˆè¯­ä¹‰å—æŠ½å–ã€‚
#### HTML æ–‡æ¡£
   - ä½¿ç”¨ BeautifulSoup é€’å½’è§£æ DOM æ ‘ï¼Œåˆ é™¤å†—ä½™èŠ‚ç‚¹ï¼ˆstyle/script/commentï¼‰ï¼›  
   - ä¸º block å…ƒç´ ç”Ÿæˆå”¯ä¸€ block_idï¼Œä¿ç•™è¡¨æ ¼ä¸æ ‡é¢˜ç»“æ„ï¼›  
   - å°†æ ‡é¢˜è½¬ Markdown è¯­æ³•ï¼ˆå¦‚ `<h1>` â†’ `#`ï¼‰ï¼Œå†æŒ‰ token æ•°åˆ‡å—ã€‚ 
   
**è®¾è®¡äº®ç‚¹**
- **å—çº§ ID ä¸æ ‡é¢˜ä¿ç•™æœºåˆ¶**ï¼šä¸º HTML æ¯ä¸ªè¯­ä¹‰å—ç”Ÿæˆ `block_id`ï¼Œå¹¶ä¿ç•™æ ‡é¢˜ç­‰çº§ä¿¡æ¯ï¼Œç¡®ä¿åˆ‡å—åä»èƒ½é‡æ„åŸæ–‡é€»è¾‘ã€‚

#### JSON æ–‡æ¡£
   - è‡ªåŠ¨è¯†åˆ« JSON / JSONL æ ¼å¼ï¼›  
   - é€šè¿‡ `_list_to_dict_preprocessing` å°†åµŒå¥— list è½¬æ¢ä¸ºå¯éå† dictï¼›  
   - æ ¹æ® `max_chunk_size` / `min_chunk_size` åŠ¨æ€ç”Ÿæˆå±‚çº§åŒ– chunkã€‚

**è®¾è®¡äº®ç‚¹**
- **æ·±åº¦ä¼˜å…ˆéå†å‰ç½®å¤„ç† JSON ç»“æ„**ï¼šå°† JSON ä¸­ value æ˜¯åˆ—è¡¨å’Œå­—å…¸çš„å¤æ‚åœºæ™¯è½¬æ¢æˆç»Ÿä¸€ç»“æ„ï¼Œä¸ºåç»­ç»Ÿä¸€å¤„ç†åšåŸºç¡€ã€‚

#### DOC æ–‡æ¡£
   - è°ƒç”¨ Apache Tika æå–æ–‡æœ¬å†…å®¹ï¼›  
   - æŒ‰æ¢è¡Œç¬¦å¿«é€Ÿåˆ‡åˆ†ï¼Œå½¢æˆæœ€å°è¯­ä¹‰å•å…ƒã€‚  



# æ‰‹æ’•ç‰ˆ

## HTML æ–‡æ¡£
1. è§£æå™¨åˆå§‹åŒ–ï¼Œè°ƒç”¨ç±»å®ä¾‹è§£æ html æ–‡æ¡£
```python
sections = HtmlParser()(filename, binary, chunk_token_num)
```

### HtmlParser ç±»
æ¨æ–­æ­£ç¡®ç¼–ç è¿›è¡Œè§£ç åï¼Œè§£æ html æ–‡ä»¶ã€‚

```python
class RAGFlowHtmlParser:
    def __call__(self, fnm, binary=None, chunk_token_num=512):
        if binary:
            encoding = find_codec(binary)
            txt = binary.decode(encoding, errors="ignore")
        else:
            with open(fnm, "r",encoding=get_encoding(fnm)) as f:
                txt = f.read()
        return self.parser_txt(txt, chunk_token_num)
```

#### parser_txt
`parser_txt` æ˜¯è§£æ html æ–‡æ¡£çš„æ ¸å¿ƒæ–¹æ³•ï¼Œä¸»è¦åˆ† 4 ä¸ªæ­¥éª¤ï¼š
1. ç§»é™¤æ–‡æ¡£å¹²æ‰°ä¿¡æ¯
```python
# åˆ é™¤ style å’Œ script æ ‡ç­¾
for style_tag in soup.find_all(["style", "script"]):
    style_tag.decompose()
# åˆ é™¤ div ä¸­çš„ script æ ‡ç­¾
for div_tag in soup.find_all("div"):
    for script_tag in div_tag.find_all("script"):
        script_tag.decompose()
# åˆ é™¤å†…è”æ ·å¼
for tag in soup.find_all(True):
    if 'style' in tag.attrs:
        del tag.attrs['style']
# åˆ é™¤ HTML æ³¨é‡Š
for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
    comment.extract()
```
2. é€’å½’æå–æ–‡æœ¬
```python
cls.read_text_recursively(soup.body, temp_sections, chunk_token_num=chunk_token_num)

@classmethod
def read_text_recursively(cls, element, parser_result, chunk_token_num=512, parent_name=None, block_id=None):
    if isinstance(element, NavigableString): # åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡æœ¬èŠ‚ç‚¹ NavigableString
        ...
        if is_valid_html(content): # åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆ html èŠ‚ç‚¹
            soup = BeautifulSoup(content, "html.parser")
            # é€’å½’è·å–å­èŠ‚ç‚¹æ–‡æœ¬å†…å®¹
            child_info = cls.read_text_recursively(soup, parser_result, chunk_token_num, element.name, block_id)
            parser_result.extend(child_info)
        else:
            # æå–æ–‡æœ¬
            info = {"content": element.strip(), "tag_name": "inner_text", "metadata": {"block_id": block_id}}
            if parent_name:
                info["tag_name"] = parent_name
            return_info.append(info)
        ...
    elif isinstance(element, Tag): # åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡ç­¾èŠ‚ç‚¹
        if str.lower(element.name) == "table": # è¡¨æ ¼èŠ‚ç‚¹ç‰¹æ®Šå¤„ç†
            table_info_list = []
            table_id = str(uuid.uuid1())
            table_list = [html.unescape(str(element))]
            for t in table_list:
                table_info_list.append({"content": t, "tag_name": "table",
                                        "metadata": {"table_id": table_id, "index": table_list.index(t)}})
            return table_info_list
        else: # ä¿åº•å¤„ç†ï¼Œè¯†åˆ«æ‰€æœ‰ html æ ‡ç­¾ BLOCK_TAGS é›†åˆ
            block_id = None
            if str.lower(element.name) in BLOCK_TAGS:
                block_id = str(uuid.uuid1())
            for child in element.children:
                child_info = cls.read_text_recursively(child, parser_result, chunk_token_num, element.name,
                                                        block_id)
                parser_result.extend(child_info)
    ...
```
>*è¡¨æ ¼èŠ‚ç‚¹ç‰¹æ®Šå¤„ç†ï¼šä¿ç•™è¡¨æ ¼åŸå§‹æ ·å¼æ ‡ç­¾ï¼Œä¸ä¼šè¿›è¡Œæ–‡æœ¬æå–ã€‚å¦‚ï¼š"content": â€œ`<table border="1"><tr><th>å§“å</th><th>å¹´é¾„</th></tr><tr><td>å¼ ä¸‰</td><td>25</td></tr><tr><td>æå››</td><td>30</td></tr></table>`â€*,

>*æå–æ–‡æœ¬è¿”å›ç»“æ„ä¸­è®¾è®¡ block id ä¿¡æ¯ï¼Œç”¨äºåç»­åˆå¹¶æ–‡æœ¬*
3. æ–‡æœ¬åˆå¹¶
```python
block_txt_list, table_list = cls.merge_block_text(temp_sections)

@classmethod
def merge_block_text(cls, parser_result):
    ...
    if block_id:
        if title_flag:
            content = f"{TITLE_TAGS[tag_name]} {content}" #æ ‡é¢˜æ·»åŠ  Markdown æ ¼å¼
        if lask_block_id != block_id:
            if lask_block_id is not None:
                block_content.append(current_content)
            current_content = content
            lask_block_id = block_id
        else:
            current_content += (" " if current_content else "") + content
    else:
        if tag_name == "table":
            table_info_list.append(item)
        else:
            current_content += (" " if current_content else "" + content)
```
>*æ ¹æ® block id åˆå¹¶æ–‡æœ¬ï¼Œå¯¹äºè¡¨æ ¼ä»ä¿æŒåŸå§‹ç»“æ„å®Œæ•´ã€‚*

>*å°†æ ‡é¢˜è½¬æ¢æˆ Markdown æ ¼å¼ï¼ˆTITLE_TAGS = {"h1": "#", "h2": "##", "h3": "###", "h4": "#####", "h5": "#####", "h6": "######"}ï¼‰ä¿ç•™è¯­ä¹‰ä¿¡æ¯*
4. æ–‡æœ¬åˆ‡åˆ†
æŒ‰ `chunk_token_num` é…ç½®åˆ‡åˆ†æ–‡æœ¬
```python
sections = cls.chunk_block(block_txt_list, chunk_token_num=chunk_token_num)

@classmethod
    def chunk_block(cls, block_txt_list, chunk_token_num=512):
        ...
```

## JSON æ–‡æ¡£
1. è§£æå™¨åˆå§‹åŒ–ï¼Œè°ƒç”¨ç±»å®ä¾‹è§£æ JSON æ–‡æ¡£
```python
sections = JsonParser(chunk_token_num)(binary)
```

### JsonParser ç±»
æ¨æ–­æ­£ç¡®ç¼–ç è¿›è¡Œè§£ç åï¼Œè§£æ JSON æ–‡ä»¶ï¼Œå…¼å®¹æ™®é€š JSON æ ¼å¼å’Œ JSONL æ ¼å¼ã€‚
```python
def __call__(self, binary):
    encoding = find_codec(binary)
    txt = binary.decode(encoding, errors="ignore")

    if self.is_jsonl_format(txt):
        sections = self._parse_jsonl(txt)
    else:
        sections = self._parse_json(txt)
    return sections
```
å¯¹äº JSONL çš„å¤„ç†æ˜¯å…ˆå°† JSONL æ¯è¡Œè½¬æ¢æˆ JSON æ ¼å¼åè°ƒç”¨ `split_json` å¤„ç†ã€‚
```python
 def _parse_jsonl(self, content: str) -> list[str]:
    lines = content.strip().splitlines()
    all_chunks = []
    for line in lines:
        if not line.strip():
            continue
        try:
            data = json.loads(line)
            chunks = self.split_json(data, convert_lists=True)
    ...
```

#### split_json
1. æ•°æ®ç»“æ„è½¬æ¢ï¼Œé€’å½’å°† JSON ä¸­åˆ—è¡¨å€¼è½¬æ¢æˆå­—å…¸ã€‚
```python
preprocessed_data = self._list_to_dict_preprocessing(json_data)

def _list_to_dict_preprocessing(self, data: Any) -> Any:
    if isinstance(data, dict):
        # Process each key-value pair in the dictionary
        return {k: self._list_to_dict_preprocessing(v) for k, v in data.items()}
    elif isinstance(data, list):
        # Convert the list to a dictionary with index-based keys
        return {str(i): self._list_to_dict_preprocessing(item) for i, item in enumerate(data)}
    else:
        # Base case: the item is neither a dict nor a list, so return it unchanged
        return data
```
é€’å½’è½¬æ¢ç¤ºä¾‹ï¼š
```python
# è¾“å…¥
{"a": [1, 2, 3], "b": {"c": ["x", "y"]}}

# è¾“å‡º
{
  "a": {
    "0": 1,
    "1": 2, 
    "2": 3
  },
  "b": {
    "c": {
      "0": "x",
      "1": "y"
    }
  }
}
```
2. æŒ‰ç…§é•¿åº¦é…ç½®å¯¹ JSON è¿›è¡Œåˆ‡åˆ†ï¼Œè¾“å‡º chunkã€‚
```python
chunks = self._json_split(preprocessed_data, None, None)

def _json_split(self, data, current_path: list[str] | None, chunks: list[dict] | None) -> list[dict]:
    """
    Split json into maximum size dictionaries while preserving structure.
    """
    ...
    for key, value in data.items():
        new_path = current_path + [key]
        chunk_size = self._json_size(chunks[-1])
        size = self._json_size({key: value})
        remaining = self.max_chunk_size - chunk_size

        if size < remaining:
            # Add item to current chunk
            self._set_nested_dict(chunks[-1], new_path, value)
        else:
            if chunk_size >= self.min_chunk_size:
                # Chunk is big enough, start a new chunk
                chunks.append({})

            # Iterate
            self._json_split(value, new_path, chunks)
    ...
```
åˆ‡åˆ†ç¤ºä¾‹ï¼š
```python
# è¾“å…¥
{"a": 1, "b": "hello", "c": {"d": 2, "e": "world"}}

#è¾“å‡ºï¼ˆå…·ä½“è¾“å‡ºéœ€è¦æ ¹æ® max_chunk_size ä¸ min_chunk_size é…ç½®ï¼‰
[
  {
    "a": 1,
    "b": "hello"
  },
  {
    "c": {
      "d": 2,
      "e": "world"
    }
  }
]
```
>*å¦‚æœå¤šå±‚åµŒå¥—æ•°æ®ï¼Œä½¿ç”¨ new_path = current_path + [key] æ¥ç»´æŠ¤åµŒå¥—å±‚çº§ï¼Œä¼˜å…ˆå¯¹ JSON æ•°æ®è¿›è¡Œæ·±åº¦éå†*
```python
@staticmethod
def _set_nested_dict(d: dict, path: list[str], value: Any) -> None:
    """Set a value in a nested dictionary based on the given path."""
    for key in path[:-1]:
        d = d.setdefault(key, {})
    d[path[-1]] = value
```

## DOC æ–‡æ¡£
å…¼å®¹æ—§ç‰ˆ word .doc æ–‡æ¡£è§£æã€‚ä½¿ç”¨ python Tika åº“è§£æ doc æ–‡æ¡£ã€‚
```python
 elif re.search(r"\.doc$", filename, re.IGNORECASE):
    doc_parsed = parser.from_buffer(binary)
    sections = doc_parsed['content'].split('\n')
    sections = [(_, "") for _ in sections if _]
```

HTMLï¼ŒJSONï¼ŒDOC æ–‡æ¡£ç»è¿‡åˆ‡åˆ†å¾—åˆ° sections åï¼Œè¿˜éœ€è¦è¿›è¡Œ sections åå¤„ç†ï¼Œè¿™éƒ¨åˆ†å¯å‚è€ƒã€Šnaive parser è¯­ä¹‰åˆ‡å—ï¼ˆpdf ç¯‡ï¼‰ã€‹ä¸­ **sections åå¤„ç†æ¨¡å—ä¸­çš„æ— å›¾ sections å¤„ç†é€»è¾‘**ï¼Œç»è¿‡åå¤„ç†åå¾—åˆ°æœ€ç»ˆè¾“å‡ºçš„ resã€‚

# ä¸‹æœŸé¢„å‘Š
åœ¨æœ¬æœŸã€Šã€è§£å¯†æºç ã€‘ RAGFlow åˆ‡åˆ†æœ€ä½³å®è·µ- naive parser è¯­ä¹‰åˆ‡å—ï¼ˆhtml & json & doc ç¯‡ï¼‰ã€‹ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥å‰–æäº† html|json|doc æ–‡æ¡£ RAGFlow ä¸­çš„å®Œæ•´è§£ææµæ°´çº¿ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æ–‡æ¡£ç±»å‹çš„è§£ææ–¹æ¡ˆï¼Œå› ä¸ºå¤©ç”Ÿå…·æœ‰ç»“æ„åŒ–ä¿¡æ¯ï¼Œè¿™å‡ ç§æ–‡æ¡£çš„è§£ææ–¹æ¡ˆæ›´åŠ ç®€å•é«˜æ•ˆã€‚

è‡³æ­¤ï¼Œnaive æ¨¡å¼ä¸‹æ‰€æœ‰æ–‡æ¡£æ ¼å¼çš„è§£ææ–¹æ¡ˆå·²ç»å…¨éƒ¨æ‹†è§£å®Œæ¯•ï¼Œä¸€å…±æ˜¯ä»¥ä¸‹ 8 ä¸­æ–‡æ¡£ç±»å‹ã€‚
```python
if re.search(r"\.docx$", filename, re.IGNORECASE):
		...
elif re.search(r"\.pdf$", filename, re.IGNORECASE):
		...
elif re.search(r"\.(csv|xlsx?)$", filename, re.IGNORECASE):
		...
elif re.search(r"\.(txt|py|js|java|c|cpp|h|php|go|ts|sh|cs|kt|sql)$", filename, re.IGNORECASE):
		...
elif re.search(r"\.(md|markdown)$", filename, re.IGNORECASE):
		...
elif re.search(r"\.(htm|html)$", filename, re.IGNORECASE):
		...
elif re.search(r"\.(json|jsonl|ldjson)$", filename, re.IGNORECASE):
		...
elif re.search(r"\.doc$", filename, re.IGNORECASE):
		...
else:
	  raise NotImplementedError(
	      "file type not supported yet(pdf, xlsx, doc, docx, txt supported)")
```
æ’’èŠ±ingğŸ‰ğŸ‰ğŸ‰