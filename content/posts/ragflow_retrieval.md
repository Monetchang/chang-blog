---
title: "ã€è§£å¯†æºç ã€‘ RAGFlow å¬å›ç­–ç•¥å…¨è§£"
date: 2025-10-16T20:39:10+08:00
draft: false
tags: ["æºç ","æŠ€æœ¯","RAG"]
categories: ["RAGFlow"]
---

# å¼•è¨€
åœ¨ RAG ç³»ç»Ÿä¸­ï¼Œ**å¬å›ç­–ç•¥**å†³å®šäº†çŸ¥è¯†æ£€ç´¢çš„ç²¾åº¦ä¸æ•ˆç‡ï¼Œæ˜¯æ•´ä¸ªé—®ç­”é“¾è·¯çš„â€œå…¥å£é€»è¾‘â€ã€‚  
RAGFlow çš„å¬å›æ¨¡å—å¹¶éç®€å•çš„å‘é‡æ£€ç´¢ï¼Œè€Œæ˜¯é›†æˆäº† **å‚æ•°è§£æ â†’ æ¨¡å‹ä¸€è‡´æ€§æ ¡éªŒ â†’ æŸ¥è¯¢å¢å¼º â†’ æ··åˆå¬å› â†’ åŠ¨æ€é‡æ’åº â†’ é˜ˆå€¼è¿‡æ»¤ä¸åˆ†é¡µ** çš„å®Œæ•´é—­ç¯ã€‚  
æœ¬æ–‡å°†æ·±å…¥è§£æ RAGFlow çš„å¬å›æºç ï¼Œæ‹†è§£å…¶ä»ç”¨æˆ·è¯·æ±‚åˆ°æœ€ç»ˆå€™é€‰æ–‡æ¡£ç”Ÿæˆçš„å…¨è¿‡ç¨‹ï¼Œæ­ç¤ºå®ƒåœ¨å·¥ç¨‹å±‚é¢å¦‚ä½•å¹³è¡¡**ç²¾å‡†æ€§ã€ç¨³å®šæ€§ä¸æ‰©å±•æ€§**ã€‚

# çœæµç‰ˆ
## æ ¸å¿ƒæ€è·¯
RAGFlow çš„å¬å›é€»è¾‘å›´ç»•â€œ**ç²¾å‡†åŒ¹é…ã€å¤šæ¨¡èåˆã€åŠ¨æ€ä¼˜åŒ–**â€å±•å¼€ã€‚  
é€šè¿‡å°† **ç¨€ç–æ£€ç´¢ï¼ˆæ–‡æœ¬åŒ¹é…ï¼‰** ä¸ **ç¨ å¯†æ£€ç´¢ï¼ˆå‘é‡åŒ¹é…ï¼‰** èåˆï¼Œå†ç»“åˆ rerank æ¨¡å‹å’Œç‰¹å¾æ‰“åˆ†æœºåˆ¶ï¼Œå®ç°æ›´æ™ºèƒ½çš„ä¸Šä¸‹æ–‡å¬å›ã€‚  
å½“æ— ç»“æœæ—¶ï¼Œç³»ç»Ÿè¿˜ä¼šè‡ªåŠ¨è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼å’ŒåŒ¹é…èŒƒå›´ï¼Œå®ç°â€œå®½æœå…œåº•â€ã€‚

### ğŸ’¡ è®¾è®¡äº®ç‚¹
1. **ç»Ÿä¸€æ¨¡å‹ä¸€è‡´æ€§æ£€æµ‹**  
   - å¤šçŸ¥è¯†åº“æŸ¥è¯¢æ—¶å¼ºåˆ¶æ ¡éªŒ Embedding ä¸€è‡´æ€§ï¼Œé¿å…è¯­ä¹‰ç©ºé—´é”™ä½ã€‚  
2. **æŸ¥è¯¢å¢å¼ºï¼ˆQuery Boostingï¼‰**  
   - é€šè¿‡ Chat æ¨¡å‹è¿›è¡Œå…³é”®è¯æ‰©å±•ï¼Œæé«˜å¤šä¹‰å¥å’Œå£è¯­åŒ–æŸ¥è¯¢çš„ç†è§£åŠ›ã€‚  
3. **èåˆæ£€ç´¢ç­–ç•¥**  
   - ç¨€ç– + ç¨ å¯†åŒé€šé“æ£€ç´¢ï¼›é»˜è®¤æŒ‰ 5% æ–‡æœ¬ç›¸ä¼¼åº¦ + 95% å‘é‡ç›¸ä¼¼åº¦åŠ æƒã€‚  
4. **åŠ¨æ€é‡æ’åºæœºåˆ¶**  
   - æ”¯æŒæŒ‡å®š rerank æ¨¡å‹æˆ–åŸºäº ES / Infinity çš„è‡ªåŠ¨å½’ä¸€åŒ–é‡æ’ã€‚  
5. **ç©ºç»“æœè‡ªé€‚åº”å›é€€**  
   - æ™ºèƒ½é™ä½é˜ˆå€¼æˆ–åˆ‡æ¢ç®€å•æ£€ç´¢æ¨¡å¼ï¼Œç¡®ä¿ç³»ç»Ÿâ€œæ°¸ä¸ç©ºç­”â€ã€‚  
6. **å¤šç»´ç‰¹å¾èåˆæ’åº**  
   - å°†æ–‡æœ¬ç›¸ä¼¼åº¦ã€å‘é‡è·ç¦»ã€PageRankã€æ ‡ç­¾ç‰¹å¾èåˆæˆæœ€ç»ˆå¾—åˆ†ï¼Œå®ç°æ›´ç¨³å¥çš„æ’åºè¾“å‡ºã€‚  

# æ‰‹æ’•ç‰ˆ

## 1. å¬å›å‚æ•°å¤„ç†
### 1.1 ç”¨æˆ· token é‰´æƒ
```python
token = request.headers.get('Authorization').split()[1]
objs = APIToken.query(token=token)
if not objs:
    return get_json_result(
        data=False, message='Authentication error: API key is invalid!"', code=settings.RetCode.AUTHENTICATION_ERROR)
```
### 1.2 è¯·æ±‚å‚æ•°è§£æ
```python
req = request.json
kb_ids = req.get("kb_id", [])                    # çŸ¥è¯†åº“IDåˆ—è¡¨
doc_ids = req.get("doc_ids", [])                 # æŒ‡å®šæ–‡æ¡£IDè¿‡æ»¤
question = req.get("question")                   # ç”¨æˆ·æŸ¥è¯¢é—®é¢˜
page = int(req.get("page", 1))                   # é¡µç ï¼Œé»˜è®¤ç¬¬1é¡µ
size = int(req.get("page_size", 30))             # æ¯é¡µå¤§å°ï¼Œé»˜è®¤30æ¡
similarity_threshold = float(req.get("similarity_threshold", 0.2))  # ç›¸ä¼¼åº¦é˜ˆå€¼
vector_similarity_weight = float(req.get("vector_similarity_weight", 0.3))  # å‘é‡æƒé‡
top = int(req.get("top_k", 1024))                # åˆå§‹å¬å›æ•°é‡
highlight = bool(req.get("highlight", False))    # æ˜¯å¦é«˜äº®åŒ¹é…å†…å®¹
```
### 1.3 æ¨¡å‹ä¸€è‡´æ€§åˆ¤æ–­
åœ¨ RAGFlow ä¸­ï¼Œåˆ›å»ºçŸ¥è¯†åº“éœ€è¦é…ç½®ç›¸åº” Embedding modelï¼Œè¿™é‡Œéœ€è¦æ£€æŸ¥æ‰€æœ‰æŸ¥è¯¢çš„çŸ¥è¯†åº“ä½¿ç”¨ç›¸åŒçš„ Embedding modelï¼Œé¿å…ä¸åŒæ¨¡å‹äº§ç”Ÿçš„å‘é‡ç©ºé—´ä¸ä¸€è‡´é—®é¢˜ã€‚
```python
kbs = KnowledgebaseService.get_by_ids(kb_ids)
embd_nms = list(set([kb.embd_id for kb in kbs]))
if len(embd_nms) != 1:
    return get_json_result(
        data=False, message='Knowledge bases use different embedding models or does not exist."',
        code=settings.RetCode.AUTHENTICATION_ERROR)

```
### 1.4 æ¨¡å‹åˆå§‹åŒ–
åˆå§‹åŒ– Embedding modelï¼Œrerank modelï¼Œå’Œ chat modelã€‚
```python
embd_mdl = LLMBundle(kbs[0].tenant_id, LLMType.EMBEDDING, llm_name=kbs[0].embd_id)
rerank_mdl = None
if req.get("rerank_id"):
    rerank_mdl = LLMBundle(kbs[0].tenant_id, LLMType.RERANK, llm_name=req["rerank_id"])
if req.get("keyword", False):
    chat_mdl = LLMBundle(kbs[0].tenant_id, LLMType.CHAT)
```

### 1.5 æŸ¥è¯¢å¢å¼ºå¤„ç†
é€šè¿‡ chat model é’ˆå¯¹æŸ¥è¯¢è¿›è¡Œè¯­ä¹‰å¢å¼º
```python
if req.get("keyword", False):
    chat_mdl = LLMBundle(kbs[0].tenant_id, LLMType.CHAT)
    question += keyword_extraction(chat_mdl, question)

# prompt
## Role
You are a text analyzer.

## Task
Extract the most important keywords/phrases of a given piece of text content.

## Requirements
- Summarize the text content, and give the top {{ topn }} important keywords/phrases.
- The keywords MUST be in the same language as the given piece of text content.
- The keywords are delimited by ENGLISH COMMA.
- Output keywords ONLY.

---

## Text Content
{{ content }}
```


## 2. æ‰§è¡Œå¬å›

### 2.1 æ„å»ºå¬å›å‚æ•°
```python
RERANK_LIMIT = math.ceil(64/page_size) * page_size if page_size>1 else 1
req = {"kb_ids": kb_ids, "doc_ids": doc_ids, "page": math.ceil(page_size*page/RERANK_LIMIT), "size": RERANK_LIMIT,
        "question": question, "vector": True, "topk": top,
        "similarity": similarity_threshold,
        "available_int": 1}
```

### 2.2 æ‰§è¡Œå¬å›
```python
sres = self.search(req, [index_name(tid) for tid in tenant_ids],
                    kb_ids, embd_mdl, highlight, rank_feature=rank_feature)
```

### 2.3 å¬å›é…ç½®åˆå§‹åŒ–
```python
def search(self, req, idx_names: str | list[str],
           kb_ids: list[str],
           emb_mdl=None,
           highlight: bool | list | None = None,
           rank_feature: dict | None = None):
    
    if highlight is None:
        highlight = False

    filters = self.get_filters(req)  # è·å–è¿‡æ»¤æ¡ä»¶
    orderBy = OrderByExpr()  # æ’åºè¡¨è¾¾å¼

    pg = int(req.get("page", 1)) - 1 # é¡µç ï¼ˆä»0å¼€å§‹ï¼‰
    topk = int(req.get("topk", 1024)) # åˆå§‹å¬å›æ•°é‡
    ps = int(req.get("size", topk)) # è¿”å›ç»“æœå¤§å°
    offset, limit = pg * ps, ps # è®¡ç®—åç§»é‡
    # é»˜è®¤è¿”å›å­—æ®µåˆ—è¡¨
    src = req.get("fields",
                    ["docnm_kwd", "content_ltks", "kb_id", "img_id", "title_tks", "important_kwd", "position_int",
                    "doc_id", "page_num_int", "top_int", "create_timestamp_flt", "knowledge_graph_kwd",
                    "question_kwd", "question_tks", "doc_type_kwd",
                    "available_int", "content_with_weight", PAGERANK_FLD, TAG_FLD])
```

### 2.4 æ— æŸ¥è¯¢è¯çš„ç®€å•æœç´¢
é’ˆå¯¹æ²¡æœ‰æŸ¥è¯¢é—®é¢˜çš„åœºæ™¯ï¼Œè¿”å›å¯¹åº”çš„ topk chunkã€‚
```python
qst = req.get("question", "")

if not qst:
    if req.get("sort"):
        orderBy.asc("page_num_int")        
        orderBy.asc("top_int")  
        orderBy.desc("create_timestamp_flt")
    
    # æ‰§è¡Œæ— æŸ¥è¯¢æ¡ä»¶çš„æœç´¢
    res = self.dataStore.search(src, [], filters, [], orderBy, offset, limit, idx_names, kb_ids)
    total = self.dataStore.getTotal(res)
```

### 2.5 æœ‰æŸ¥è¯¢è¯çš„æ™ºèƒ½æœç´¢
#### 2.5.1 è§£ææŸ¥è¯¢é—®é¢˜
```python
matchText, keywords = self.qryr.question(qst, min_match=0.3)

def question(self, txt, tbl="qa", min_match: float = 0.6):
    ...
```
**1ï¼‰è§„èŒƒæŸ¥è¯¢é—®é¢˜æ–‡æœ¬æ ¼å¼**

**åœ¨è‹±æ–‡å’Œä¸­æ–‡æ–‡æœ¬ä¹‹é—´è‡ªåŠ¨æ·»åŠ ç©ºæ ¼ï¼Œä½¿æ–‡æœ¬æ ¼å¼æ›´åŠ è§„èŒƒï¼Œæé«˜å¯è¯»æ€§ã€‚**
```python
txt = FulltextQueryer.add_space_between_eng_zh(txt)

def add_space_between_eng_zh(txt):
    # (ENG/ENG+NUM) + ZH
    txt = re.sub(r'([A-Za-z]+[0-9]+)([\u4e00-\u9fa5]+)', r'\1 \2', txt)
    # ENG + ZH
    txt = re.sub(r'([A-Za-z])([\u4e00-\u9fa5]+)', r'\1 \2', txt)
    # ZH + (ENG/ENG+NUM)
    txt = re.sub(r'([\u4e00-\u9fa5]+)([A-Za-z]+[0-9]+)', r'\1 \2', txt)
    txt = re.sub(r'([\u4e00-\u9fa5]+)([A-Za-z])', r'\1 \2', txt)
    return txt
```
**å…¨è§’è½¬åŠè§’ã€ç¹ä½“è½¬ç®€ä½“ã€å°å†™ã€å»é™¤æ ‡ç‚¹ã€‚**
```python
txt = re.sub(
    r"[ :|\r\n\t,ï¼Œã€‚ï¼Ÿ?/`!ï¼&^%%()\[\]{}<>]+",
    " ",
    rag_tokenizer.tradi2simp(rag_tokenizer.strQ2B(txt.lower())),
).strip()
```
**ç§»é™¤å¯¹æŸ¥è¯¢æ ¸å¿ƒè¯­ä¹‰å½±å“ä¸å¤§çš„è¾…åŠ©è¯æ±‡ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€‚**
```python
txt = FulltextQueryer.rmWWW(txt)
def rmWWW(txt):
    patts = [
        (
            r"æ˜¯*(æ€ä¹ˆåŠ|ä»€ä¹ˆæ ·çš„|å“ªå®¶|ä¸€ä¸‹|é‚£å®¶|è¯·é—®|å•¥æ ·|å’‹æ ·äº†|ä»€ä¹ˆæ—¶å€™|ä½•æ—¶|ä½•åœ°|ä½•äºº|æ˜¯å¦|æ˜¯ä¸æ˜¯|å¤šå°‘|å“ªé‡Œ|æ€ä¹ˆ|å“ªå„¿|æ€ä¹ˆæ ·|å¦‚ä½•|å“ªäº›|æ˜¯å•¥|å•¥æ˜¯|å•Š|å—|å‘¢|å§|å’‹|ä»€ä¹ˆ|æœ‰æ²¡æœ‰|å‘€|è°|å“ªä½|å“ªä¸ª)æ˜¯*",
            "",
        ),
        (r"(^| )(what|who|how|which|where|why)('re|'s)? ", " "),
        (
            r"(^| )('s|'re|is|are|were|was|do|does|did|don't|doesn't|didn't|has|have|be|there|you|me|your|my|mine|just|please|may|i|should|would|wouldn't|will|won't|done|go|for|with|so|the|a|an|by|i'm|it's|he's|she's|they|they're|you're|as|by|on|in|at|up|out|down|of|to|or|and|if) ",
            " ")
    ]
    otxt = txt
    for r, p in patts:
        txt = re.sub(r, p, txt, flags=re.IGNORECASE)
    if not txt:
        txt = otxt
    return txt
```

**2ï¼‰è‹±æ–‡æŸ¥è¯¢å¤„ç†**

**åˆ†è¯åè¿›è¡Œåˆ†è¯çš„æƒé‡è®¡ç®—ã€‚**
```python
if not self.isChinese(txt):
    txt = FulltextQueryer.rmWWW(txt)
    tks = rag_tokenizer.tokenize(txt).split()
    keywords = [t for t in tks if t]
    # åˆ†è¯æƒé‡è®¡ç®—
    tks_w = self.tw.weights(tks, preprocess=False)
    tks_w = [(re.sub(r"[ \\\"'^]", "", tk), w) for tk, w in tks_w]
    tks_w = [(re.sub(r"^[a-z0-9]$", "", tk), w) for tk, w in tks_w if tk]
    tks_w = [(re.sub(r"^[\+-]", "", tk), w) for tk, w in tks_w if tk]
    tks_w = [(tk.strip(), w) for tk, w in tks_w if tk.strip()]
```
`self.tw.weights` æ˜¯è®¡ç®—åˆ†è¯æƒé‡çš„æ ¸å¿ƒæ–¹æ³•ã€‚è¿™é‡Œç®€å•ä»‹ç»ä¸€ä¸‹å…¶ä¸­ä½¿ç”¨åˆ°çš„æƒé‡ç­–ç•¥ã€‚
- `ner(t)`ï¼šæ ¹æ®åˆ†è¯æ˜¯å¦ä¸ºæ•°å­—ã€çŸ­å­—æ¯æˆ–ç‰¹å®šç±»å‹çš„å‘½åå®ä½“ï¼ˆå¦‚å…¬å¸åã€åœ°åã€å­¦æ ¡åç­‰ï¼‰åˆ†é…ä¸åŒæƒé‡ã€‚
- `postag(t)`ï¼šæ ¹æ®åˆ†è¯çš„è¯æ€§ï¼ˆè¿è¯ï¼Œä»£è¯ç­‰ï¼‰åˆ†é…ä¸åŒæƒé‡ï¼Œåæ˜ ä¸åŒè¯æ€§åœ¨æ–‡æœ¬ä¸­çš„é‡è¦æ€§å·®å¼‚ã€‚
- `freq(t)`ï¼šè®¡ç®—åˆ†è¯çš„é¢‘ç‡ç‰¹å¾ï¼Œå¹¶å¯¹æœªè¯†åˆ«çš„åˆ†è¯å’Œé•¿è¯è¿›è¡Œç‰¹æ®Šå¤„ç†ã€‚
- `df(t)`ï¼šè®¡ç®—åˆ†è¯åœ¨æ–‡æ¡£é›†åˆä¸­çš„åˆ†å¸ƒæƒ…å†µï¼Œåæ˜ åˆ†è¯çš„æ–‡æ¡£é—´åŒºåˆ†åº¦ã€‚
- `idf(s, N)`ï¼šç»å…¸çš„ IDF è®¡ç®—å…¬å¼ï¼Œç”¨äºè¡¡é‡åˆ†è¯çš„é‡è¦æ€§ï¼Œå…¶ä¸­ N æ˜¯æ–‡æ¡£æ€»æ•°ï¼Œs æ˜¯åŒ…å«è¯¥åˆ†è¯çš„æ–‡æ¡£æ•°ã€‚
  
æƒé‡è®¡ç®—è¾“å‡ºç¤ºä¾‹
```
[('Chinese', 0.35), ('economy', 0.28), ('development', 0.22), ('quickly', 0.15)]
```

**åˆ†è¯åŒä¹‰è¯æ‰©å±•ï¼Œå¹¶èµ‹äºˆ 1/4 åˆ†è¯æƒé‡ã€‚**
```python
for tk, w in tks_w[:256]:
    # åˆ†è¯åŒä¹‰è¯æŸ¥è¯¢
    syn = self.syn.lookup(tk)
    syn = rag_tokenizer.tokenize(" ".join(syn)).split()
    keywords.extend(syn)
    syn = ["\"{}\"^{:.4f}".format(s, w / 4.) for s in syn if s.strip()]
    syns.append(" ".join(syn))
```
`self.syn.lookup(tk)` æ˜¯å®ç°åˆ†è¯åŒä¹‰è¯æŸ¥æ‰¾çš„æ ¸å¿ƒæ–¹æ³•ã€‚ä¸»è¦é€šè¿‡è¯è¡¨æ¥è¿›è¡ŒåŒä¹‰è¯çš„æŸ¥è¯¢ï¼Œè‹±æ–‡ä½¿ç”¨ wordnet è¯åº“ï¼Œä¸­æ–‡ä½¿ç”¨è‡ªæ„å»ºçš„è¯åº“ã€‚

**ç»“åˆæƒé‡ï¼ŒåŒä¹‰è¯ä¿¡æ¯ï¼Œæ„å»ºæŸ¥è¯¢è¡¨è¾¾å¼**
```python
# 1. å•ä¸ªè¯ + åŒä¹‰è¯æŸ¥è¯¢
q = [
    "({}^{:.4f}".format(tk, w) + " {})".format(syn) 
    for (tk, w), syn in zip(tks_w, syns) 
    if tk and not re.match(r"[.^+\(\)-]", tk)
]

# 2. ç›¸é‚»è¯çŸ­è¯­æŸ¥è¯¢ï¼ˆæå‡ç›¸é‚»è¯æƒé‡ï¼‰
for i in range(1, len(tks_w)):
    left, right = tks_w[i - 1][0].strip(), tks_w[i][0].strip()
    if not left or not right:
        continue
    q.append(
        '"%s %s"^%.4f' % (
            tks_w[i - 1][0],
            tks_w[i][0],
            max(tks_w[i - 1][1], tks_w[i][1]) * 2,  # çŸ­è¯­æƒé‡åŠ å€
        )
    )
```

**3ï¼‰æ„å»ºæœ€ç»ˆæŸ¥è¯¢å‚æ•°**
```python
return MatchTextExpr(
    self.query_fields, query, 100
), keywords

# MatchTextExpr ç»“æ„
{
    "fields": [
        "title_tks^10",
        "title_sm_tks^5",
        "important_kwd^30",
        "important_tks^20",
        "question_tks^20",
        "content_ltks^2",
        "content_sm_ltks",
    ] # åœ¨è¿™äº›å¯¹åº”å­—æ®µè¿›è¡ŒæŸ¥è¯¢
    "query": "" # æŸ¥è¯¢è¡¨è¾¾å¼
    "topn": 100 # è¿”å›ç»“æœæ•°
    "extra_options": "" # å…¶ä»–é…ç½®
}
```

**4ï¼‰ä¸­æ–‡æŸ¥è¯¢å¤„ç†**

**ä¸è‹±æ–‡çš„å¤„ç†æµç¨‹å¤§è‡´ç›¸åŒã€‚**
```python
# å‰ç½®å¤„ç†ï¼ŒæŒ‰ç…§ç©ºæ ¼å¯¹ä¸­æ–‡è¿›è¡Œåˆ†è¯
for tt in self.tw.split(txt)[:256]:
    if not tt:
        continue
    keywords.append(tt)
    # åˆ†è¯æƒé‡è®¡ç®—
    twts = self.tw.weights([tt])
    # åŒä¹‰è¯æŸ¥è¯¢
    syns = self.syn.lookup(tt)
    # é™åˆ¶åŒä¹‰è¯æŸ¥æ‰¾ç»“æœ
    if syns and len(keywords) < 32:
        keywords.extend(syns)
    tms = []
    for tk, w in sorted(twts, key=lambda x: x[1] * -1):
        sm = (
            # ç²¾ç»†åˆ†è¯ 
            rag_tokenizer.fine_grained_tokenize(tk).split()
            if need_fine_grained_tokenize(tk)
            else []
        )
    ...
    # æ„å»ºæœ€ç»ˆæŸ¥è¯¢å‚æ•°
    query = " OR ".join([f"({t})" for t in qs if t])
    if not query:
        query = otxt
    return MatchTextExpr(
        self.query_fields, query, 100, {"minimum_should_match": min_match}
    ), keywords
```

**æœ€ç»ˆæŸ¥è¯¢é—®é¢˜è§£æè¾“å‡ºï¼š**
```python
matchText, keywords = self.qryr.question(qst, min_match=0.3)
# æŸ¥è¯¢ä¿¡æ¯ç»“æ„ä½“ï¼ŒåŒä¸Šæ–¹ MatchTextExpr ç»“æ„
{
    "fields": [
        "title_tks^10",
        "title_sm_tks^5",
        "important_kwd^30",
        "important_tks^20",
        "question_tks^20",
        "content_ltks^2",
        "content_sm_ltks",
    ] # åœ¨è¿™äº›å¯¹åº”å­—æ®µè¿›è¡ŒæŸ¥è¯¢
    "query": "" # æŸ¥è¯¢è¡¨è¾¾å¼
    "topn": 100 # è¿”å›ç»“æœæ•°
    "extra_options": "" # å…¶ä»–é…ç½®
}
# keywords å…³é”®å­—åˆ—è¡¨
keywords = []
```

#### 2.5.2 ç¨€ç–æ£€ç´¢
æ²¡æœ‰è®¾ç½® Embedding modelï¼Œé€šè¿‡æŸ¥è¯¢è¯­å¥å’Œç­›é€‰é¡¹è¿›è¡Œæ£€ç´¢ã€‚
```python
if emb_mdl is None:
    # æŸ¥è¯¢è¯­å¥
    matchExprs = [matchText]
    
    res = self.dataStore.search(
        src, highlightFields, filters, matchExprs, orderBy, 
        offset, limit, idx_names, kb_ids, rank_feature=rank_feature
    )
    total = self.dataStore.getTotal(res)
```

#### 2.5.3 æ··åˆæ£€ç´¢ï¼ˆç¨ å¯†+ç¨€ç–ï¼‰
å…ˆè·å–æŸ¥è¯¢æ–‡æœ¬çš„å‘é‡è¡¨ç¤º
```python
matchDense = self.get_vector(qst, emb_mdl, topk, req.get("similarity", 0.1))
q_vec = matchDense.embedding_data
```
åˆ›å»ºæ£€ç´¢è¡¨è¾¾å¼ï¼Œåˆ†è¯åŒ¹é…æƒé‡ 5%ï¼Œå‘é‡åŒ¹é…æƒé‡ 95%ï¼Œè¿›è¡Œæ£€ç´¢ã€‚
```python
fusionExpr = FusionExpr("weighted_sum", topk, {"weights": "0.05,0.95"})
matchExprs = [matchText, matchDense, fusionExpr]
res = self.dataStore.search(src, highlightFields, filters, matchExprs, orderBy, offset, limit,
                            idx_names, kb_ids, rank_feature=rank_feature)
total = self.dataStore.getTotal(res)
```

#### 2.5.4 ç©ºç»“æœå›é€€ç­–ç•¥
å½“é€šè¿‡ä¸Šè¿°æ–¹æ¡ˆæœªæ£€ç´¢åˆ°ä»»ä½•ç»“æœï¼Œåˆ™å°è¯•æ”¾å®½æ¡ä»¶é‡æ–°æœç´¢

å¦‚æœè¿‡æ»¤æ¡ä»¶ä¸­æœ‰æŒ‡å®šæ–‡æ¡£ idï¼Œåˆ™è¿›å…¥æ— æŸ¥è¯¢è¯çš„ç®€å•æœç´¢è¿”å› limit åˆ‡å—ã€‚
```python
if total == 0:
    if filters.get("doc_id"):
        res = self.dataStore.search(src, [], filters, [], orderBy, offset, limit, idx_names, kb_ids)
        total = self.dataStore.getTotal(res)
```

è°ƒæ•´åŒ¹é…é˜ˆå€¼ï¼Œåˆ†è¯åŒ¹é…åº¦ 30% -> 10%ï¼Œå‘é‡åŒ¹é…åº¦ 0.1 -> 0.17ã€‚
```python
matchText, _ = self.qryr.question(qst, min_match=0.1)
matchDense.extra_options["similarity"] = 0.17

res = self.dataStore.search(
    src, highlightFields, filters, [matchText, matchDense, fusionExpr],
    orderBy, offset, limit, idx_names, kb_ids, rank_feature=rank_feature
)
total = self.dataStore.getTotal(res)
```

## 3. é‡æ’åº
å¦‚æœæŒ‡å®šé‡æ’åºæ¨¡å‹ï¼Œæ£€ç´¢åˆ†å—æ•°å¤§äº 0ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šé‡æ’åºæ¨¡å‹å¯¹ç»“æœè¿›è¡Œé‡æ’åºã€‚
```python
if rerank_mdl and sres.total > 0:
    sim, tsim, vsim = self.rerank_by_model(rerank_mdl,
                                            sres, question, 1 - vector_similarity_weight,
                                            vector_similarity_weight,
                                            rank_feature=rank_feature)
```

å¦‚æœæ²¡æœ‰æŒ‡å®šé‡æ’åºæ¨¡å‹ï¼Œåˆ™æ ¹æ®æœç´¢å¼•æ“è¿›è¡Œé‡æ’åºé€»è¾‘
```python
lower_case_doc_engine = os.getenv('DOC_ENGINE', 'elasticsearch')
if lower_case_doc_engine == "elasticsearch":
    # ElasticSearch rerank
    sim, tsim, vsim = self.rerank(
        sres, question, 1 - vector_similarity_weight, vector_similarity_weight,
        rank_feature=rank_feature)
else:
    # Infinity åœ¨èåˆå‰ä¼šå¯¹æ¯ç§æ–¹å¼çš„åˆ†æ•°è¿›è¡Œæ ‡å‡†åŒ–ï¼Œæ‰€ä»¥è¿™é‡Œæ— éœ€é‡æ–°æ’åº
    sim = [sres.field[id].get("_score", 0.0) for id in sres.ids]
    sim = [s if s is not None else 0. for s in sim]
    tsim = sim
    vsim = sim
```

### 3.1 æŒ‡å®šæ¨¡å‹é‡æ’åº
```python
sim, tsim, vsim = self.rerank_by_model(rerank_mdl,
                                            sres, question, 1 - vector_similarity_weight,
                                            vector_similarity_weight,
                                            rank_feature=rank_feature)
```
#### 3.1.1 æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
```python
tksim = self.qryr.token_similarity(keywords, ins_tw)
```
é€šè¿‡ weights æ–¹æ³•è®¡ç®—æ‰€æœ‰åˆ†è¯çš„æƒé‡ï¼Œæ„æˆæŸ¥è¯¢è¯æƒé‡å­—å…¸å’Œæ–‡æ¡£è¯æƒé‡å­—å…¸åˆ—è¡¨ï¼Œweights æ–¹æ³•çš„å®ç°åœ¨è‹±æ–‡æŸ¥è¯¢å¤„ç†æœ‰è¯¦ç»†ä»‹ç»ã€‚
```python
def token_similarity(self, atks, btkss):
    def toDict(tks):
        d = defaultdict(int)
        wts = self.tw.weights(tks, preprocess=False)  # è®¡ç®—è¯æƒé‡
        for token, weight in wts:
            d[token] += weight
        return d
    
    query_dict = toDict(atks) # æŸ¥è¯¢è¯åŠ æƒå­—å…¸
    doc_dicts = [toDict(tks) for tks in btkss]  # æ–‡æ¡£è¯æƒé‡å­—å…¸åˆ—è¡¨
```
é€šè¿‡ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼ï¼šç›¸ä¼¼åº¦ = æŸ¥è¯¢è¯çš„åŒ¹é…æƒé‡/æ€»æƒé‡ï¼Œå¾—åˆ°æŸ¥è¯¢è¯å¯¹äºå„ä¸ªæ–‡æ¡£çš„ç›¸ä¼¼åº¦åˆ—è¡¨ã€‚
```python
return [self.similarity(query_dict, doc_dict) for doc_dict in doc_dicts]

def similarity(self, qtwt, dtwt):
    s = 1e-9
    for k, v in qtwt.items():
        if k in dtwt:
            s += v 
    
    q = 1e-9
    for k, v in qtwt.items():
        q += v
    
    return s / q 
```

#### 3.1.2 æŒ‡å®š rerank æ¨¡å‹ç›¸ä¼¼åº¦è®¡ç®—
```python
doc_texts = [remove_redundant_spaces(" ".join(tks)) for tks in ins_tw]
vtsim, _ = rerank_mdl.similarity(query, doc_texts)
```

#### 3.1.3 æ’åç‰¹å¾è®¡ç®—
ä¸»è¦æ˜¯åŸºäºæ ‡ç­¾ç‰¹å¾è®¡ç®—æ–‡æ¡£åŒ¹é…åº¦ï¼Œæ•´ä½“å®ç°æ¯”è¾ƒå¤æ‚ï¼Œæœ‰å…´è¶£çš„å¯ä»¥å¯¹æºç è¿›è¡Œç ”ç©¶ã€‚è¿™é‡Œç®€å•ä»‹ç»ä¸‹æ€è·¯ï¼š
- å…ˆå¯¹ç”¨æˆ·æŸ¥è¯¢è¿›è¡Œç‰¹å¾è®¡ç®—ï¼Œè®¡ç®—å‡º n ä¸ªç‰¹å¾æ ‡ç­¾ï¼Œä»¥åŠæ¯ä¸ªç‰¹å¾æ ‡ç­¾çš„æƒé‡ï¼›
- é’ˆå¯¹è¿™ n ä¸ªç‰¹å¾æ ‡ç­¾æŸ¥è¯¢å‡ºæ¯ä¸ªæ ‡ç­¾å¯¹åº”çš„æ–‡æ¡£æ•°é‡ï¼›
- æ ¹æ®ç‰¹å¾æ ‡ç­¾è‡ªèº«æƒé‡å’Œæ–‡æ¡£æ•°è®¡ç®—ç‰¹å¾æ ‡ç­¾åœ¨æ€»æ–‡æ¡£ä¹‹ä¸­çš„æƒé‡ï¼›
- å–æƒé‡å‰ 3 ä½œä¸ºç”¨æˆ·æŸ¥è¯¢ç‰¹å¾æ ‡ç­¾ï¼›
- è®¡ç®—æŸ¥è¯¢ç‰¹å¾å‘é‡çš„ L2 èŒƒæ•°ï¼Œæå– PageRank åˆ†æ•°ï¼ˆæ–‡æ¡£æƒå¨æ€§ï¼‰ï¼Œè®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„æ ‡ç­¾åŒ¹é…åº¦ï¼›
- æœ€ç»ˆåˆ†æ•°èåˆå¾—åˆ°æ’åã€‚
```python
rank_fea = self._rank_feature_scores(rank_feature, sres)
```

#### 3.1.4 æœ€ç»ˆæ··åˆç›¸ä¼¼åº¦
æ–‡æœ¬ç›¸ä¼¼åº¦ + æ’åç‰¹å¾ï¼Œç„¶åä¸å‘é‡ç›¸ä¼¼åº¦åŠ æƒèåˆ
```python
final_scores = tkweight * (np.array(tksim) + rank_fea) + vtweight * vtsim
```

### 3.2 ES é‡æ’åº
æ•´ä½“æµç¨‹å’ŒæŒ‡å®šæ¨¡å‹çš„é‡æ’åºæµç¨‹ç›¸ä¼¼ï¼Œè®¡ç®—ç›¸ä¼¼åº¦ï¼Œè®¡ç®—ç‰¹å¾ï¼Œåˆ°æœ€ç»ˆåˆ†æ•°æ’åï¼Œåœ¨è®¡ç®—ç›¸ä¼¼åº¦ä¸­ä¸æŒ‡å®šæ¨¡å‹ä¸åŒçš„æ˜¯ï¼Œå°†æŒ‡å®š rerank æ¨¡å‹å‘é‡ç›¸ä¼¼åº¦è®¡ç®—æ­¥éª¤æ›¿æ¢æˆ cos ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ã€‚
```python
sim, tsim, vsim = self.rerank(
    sres, question, 1 - vector_similarity_weight, vector_similarity_weight,
    rank_feature=rank_feature)
```

### 3.3 æœªæŒ‡å®šæ¨¡å‹é‡æ’åºï¼ˆåŸºäº Infinityï¼‰
å› ä¸º Infinity åœ¨å†…éƒ¨å·²ç»å¯¹æ–‡æœ¬æ£€ç´¢å’Œå‘é‡æ£€ç´¢çš„åˆ†æ•°è¿›è¡Œäº†å½’ä¸€åŒ–å¤„ç†ï¼Œæ‰€ä»¥ç›´æ¥èµ‹å€¼è¾“å‡ºå³å¯ã€‚
```python
# Don't need rerank here since Infinity normalizes each way score before fusion.
sim = [sres.field[id].get("_score", 0.0) for id in sres.ids]
sim = [s if s is not None else 0. for s in sim]
tsim = sim
vsim = sim
```

é‡æ’åºæµç¨‹ç»“æŸåï¼Œä¼šå¾—åˆ°ä¸‰ä¸ªç›¸ä¼¼åº¦ï¼š
- simï¼šæ··åˆç›¸ä¼¼åº¦
- tsimï¼šæ–‡æœ¬ç›¸ä¼¼åº¦
- vsimï¼šå‘é‡ç›¸ä¼¼åº¦

## 4. chunk åˆ—è¡¨æ„å»ºè¾“å‡º
### 4.1 åˆ†é¡µå’Œæ’åº
æ ¹æ®ç”¨æˆ·ä¼ å‚è¿”å›å¯¹åº”é‡æ’åºç»“æœï¼Œå¹¶è¿›è¡Œç›¸ä¼¼åº¦é™åºæ’åˆ—ã€‚
```python
max_pages = RERANK_LIMIT // page_size
page_index = (page % max_pages) - 1
begin = max(page_index * page_size, 0)
sim = sim[begin : begin + page_size]

# æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
sim_np = np.array(sim)
idx = np.argsort(sim_np * -1)
```

### 4.2 ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
æ ¹æ®è®¾ç½®çš„ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ä½äºé˜ˆå€¼çš„ç»“æœã€‚
```python
 dim = len(sres.query_vector)
    vector_column = f"q_{dim}_vec"
    zero_vector = [0.0] * dim
    filtered_count = (sim_np >= similarity_threshold).sum()
    ranks["total"] = int(filtered_count) # Convert from np.int64 to Python int otherwise JSON serializable error
    for i in idx:
        if sim[i] < similarity_threshold:
            break
```

### 4.3 æ„å»ºå•æ¡æ•°æ®ç»“æ„
```python
d = {
    "chunk_id": id,
    "content_ltks": chunk["content_ltks"],  # åˆ†è¯åçš„å†…å®¹
    "content_with_weight": chunk["content_with_weight"],  # å¸¦æƒé‡çš„å†…å®¹
    "doc_id": did,
    "docnm_kwd": dnm,
    "kb_id": chunk["kb_id"],
    "important_kwd": chunk.get("important_kwd", []),  # é‡è¦å…³é”®è¯
    "image_id": chunk.get("img_id", ""),  # å…³è”å›¾ç‰‡ID
    "similarity": sim[i],  # æœ€ç»ˆç›¸ä¼¼åº¦
    "vector_similarity": vsim[i],  # å‘é‡ç›¸ä¼¼åº¦
    "term_similarity": tsim[i],  # æ–‡æœ¬ç›¸ä¼¼åº¦
    "vector": chunk.get(vector_column, zero_vector),  # å‘é‡æ•°æ®
    "positions": position_int,  # åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®
    "doc_type_kwd": chunk.get("doc_type_kwd", "")  # æ–‡æ¡£ç±»å‹
}
```

### 4.4 è¿”å›åˆ—è¡¨
æ·»åŠ æ–‡æ¡£èšåˆä¿¡æ¯ï¼ŒæŒ‰ç…§é™åºæ’åˆ—è¾“å‡ºã€‚æ–‡æ¡£èšåˆä¿¡æ¯åŒ…å«æ¯ä¸ªæ–‡æ¡£åœ¨æœ€ç»ˆç»“æœä¸­å‡ºç°äº†å¤šå°‘ä¸ª chunkã€‚
```python
ranks["chunks"].append(d)
if dnm not in ranks["doc_aggs"]:
    ranks["doc_aggs"][dnm] = {"doc_id": did, "count": 0}
ranks["doc_aggs"][dnm]["count"] += 1
ranks["doc_aggs"] = [{"doc_name": k,
                      "doc_id": v["doc_id"],
                      "count": v["count"]} for k, v in 
                     sorted(ranks["doc_aggs"].items(),
                            key=lambda x: x[1]["count"] * -1)]  # æŒ‰counté™åº
ranks["chunks"] = ranks["chunks"][:page_size]

return ranks
```